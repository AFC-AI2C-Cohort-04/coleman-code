{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpkDf8Ca0wq5"
   },
   "source": [
    "# Project 3: Domain Data Preparation\n",
    "\n",
    "With the growth of large-scale natural language processing systems like ChatGPT, more and more groups are trying to build their own natural language processing systems for personal use. As a machine learning engineer, it is critical to not only be able to perform advanced modelling tasks, but to also critically assess the datasets that you use, especially if they rely on using data from several different domains interchangeably.\n",
    "\n",
    "In this project, we shall extract our own COVID-19 dataset from three separate sources (Twitter/X Data, News Article Data, and Research Paper Data ), and attempt to use data engineering to reduce the [Proxy A-Distance](https://papers.nips.cc/paper_files/paper/2006/hash/b1b0432ceafb0ce714426e9114852ac7-Abstract.html) between them. Reducing this distance should allow most systems trained on this data to focus on the content of the data, which we care about, instead of caring about the source of the data, which we do not care about. In doing so, we will demonstrate the importance of feature extraction when it comes to natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWjhBRVv0wrB"
   },
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PcJqDhhX0wrC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import requests\n",
    "import json, collections, time, re, string, os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pdfminer import high_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IouMqx800wrC",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will not be run by the autograder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiwO3ebC0wrI"
   },
   "source": [
    "## Part A: Social Media Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ctwt4mb0wrI"
   },
   "source": [
    "While it's current popularity has been waning, Twitter/X was one of the most popular social media platforms; according to [Statista](https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/), it has 556 million monthly active users. As tweets/posts are public information, they can provide us important hour by hour information for how the country felt during lockdown situations.\n",
    "\n",
    "In this section, you will perform basic preprocessing and feature extraction on tweet data. As we talked about in the primer, the goal with natural language processing is to convert sequences of text into vectors we can then use in any machine learning algorithm. As our goal is to ensure all three sources of data are undistinguishable by our distance metric, our goal here is to both get a glimpse of the data, and remove any obvious differences that we see.\n",
    "\n",
    "To begin - let's start by loading the twitter response dataset, and looking at a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sseLhnqr_oxp",
    "outputId": "f9fc674f-d7d7-4e81-8831-ad5bb768b498",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Good info from @FlexJobs: Create your own hand sanitizer. The recipe: https://t.co/5GLdNUNmg6 #coronavirus #coronavirusoutbreak #coronaoutbreak',\n",
       "  'lang': 'en',\n",
       "  'id': 101376,\n",
       "  'time': '2020-03-09'},\n",
       " {'text': 'Letâ€™s see, during a virus crisis  do I want the guy whoâ€™s going to battle for Medicare for all or the guy who goes meh adequate is fine??? #CoronavirusOutbreak #COVID19 #Bernie2020',\n",
       "  'lang': 'en',\n",
       "  'id': 76040,\n",
       "  'time': '2020-03-09'},\n",
       " {'text': \"Be carefull guy's and wish you all happy holi to you &amp; your family. :) \\n#HappyHoli #CoronavirusOutbreak #à¤¹à¥‹à¤²à¥€ #à¤¹à¥‹à¤²à¤¿à¤•à¤¾_à¤¦à¤¹à¤¨ #BankLooteriBJP #Coronavid19 #marketcrash  #reliance #colours #KurkureWithSidNaaz #MondayMorning #MereAngneMein ##RangBarseWithSid #à¤¬à¥�à¤°à¤¾_à¤¨_à¤®à¤¾à¤¨à¥‹_à¤¹à¥‹à¤²à¥€_à¤¹à¥ˆ https://t.co/Rg2SpMNKZD\",\n",
       "  'lang': 'en',\n",
       "  'id': 73533,\n",
       "  'time': '2020-03-09'},\n",
       " {'text': 'Latest Update on #coronavirus ðŸŒ� Wide\\n\\n3/9/2020, 6:33:16 AM\\n\\nTotal #Confirmed Cases: 110,034\\nTotal #Deaths: 3,825\\nTotal #Recovered: 61,977\\n\\nSource: Johns Hopkins university Database\\n#Coronavid19 #CoronavirusOutbreak',\n",
       "  'lang': 'en',\n",
       "  'id': 65859,\n",
       "  'time': '2020-03-09'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"twitter.txt\",'r') as mf:\n",
    "  tweet_data = json.load(mf)\n",
    "\n",
    "list((tweet_data[0],tweet_data[1000],tweet_data[8200],tweet_data[9000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uRjBrQx0wrL"
   },
   "source": [
    "### Question 1: Process tweet data\n",
    "Looking at some of the tweets above, we see that:\n",
    "1. Some tweets contain Twitter-shortened URLs, for example `https://t.co/DzhsXPxUDa`. These are always in the form of `http://t.co/` or `https://t.co/` followed by 10 alphanumeric characters. These links should be removed, as they are unlikely to be in any other data set.\n",
    "1. Some tweets contain emoticons such as `:)` or `<3`. The characters in these emoticons should be removed, as again they are unlikely to be in any other dataset.\n",
    "\n",
    "Implement the function `process_tweet` that takes as the text of a tweet, performs these two steps, removes the whitespace ahead and after the tweet, and then returns the text data altogether.\n",
    "\n",
    "**Notes**:\n",
    "* You should remove URL before removing emoticons.\n",
    "* We have provided a list of emoticons for you in the variable `emoticons`. You can assume that only elements in this set are considered emoticons and need to be removed.\n",
    "* Note that there may be no space between a shortened URL and the next word. However, you can assume that there are always 10 alphanumeric characters after http://t.co/ or https://t.co/.\n",
    "* When you finish processing the text, remember to ensure that all beginning and following whitespace is removed using `.strip()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8khGD3Wc0wrL"
   },
   "outputs": [],
   "source": [
    "emoticons = [\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3',\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', 'b=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "]\n",
    "\n",
    "\n",
    "def process_tweet(tweet_text):\n",
    "    \"\"\"\n",
    "    Process and tokenize tweets, in addition to removing URLs and emoticons\n",
    "\n",
    "    args:\n",
    "        tweet_text (str) : a list of tweet contents\n",
    "\n",
    "    return:\n",
    "        str :  the processed tweet\n",
    "    \"\"\"\n",
    "def process_tweet(tweet_text):\n",
    "    \"\"\"\n",
    "    Process and tokenize tweets, in addition to removing URLs and emoticons\n",
    "\n",
    "    args:\n",
    "        tweet_text (str) : a list of tweet contents\n",
    "\n",
    "    return:\n",
    "        str :  the processed tweet\n",
    "    \"\"\"\n",
    "    pattern_1 = r\"http://t\\.co/[a-zA-Z0-9]{10}\"\n",
    "    processed_tweet = re.sub(pattern_1, \" \", tweet_text)\n",
    "\n",
    "    pattern_2 = r\"https://t\\.co/[a-zA-Z0-9]{10}\"\n",
    "    processed_tweet = re.sub(pattern_2, \" \", processed_tweet)\n",
    "\n",
    "    for emoticon in emoticons:\n",
    "        processed_tweet = processed_tweet.replace(emoticon, \" \")\n",
    "\n",
    "    processed_tweet = processed_tweet.strip()\n",
    "    return processed_tweet\n",
    "\n",
    "# do not modify this function\n",
    "def process_tweet_data(tweet_texts):\n",
    "    return [process_tweet(tweet_text['text']) for tweet_text in tweet_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ci83YAp30wrL",
    "outputId": "9b604fda-c920-4c00-b194-f29f92dc8cd0",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_process_tweet():\n",
    "    assert process_tweet(\"It's a great day :D\") == \"It's a great day\"\n",
    "    assert process_tweet(\"<3hello\") == \"hello\"\n",
    "    assert process_tweet(\"goodX-Dday\") == \"good day\"\n",
    "    assert process_tweet(\"http://t.co/WJs5bmRthU,http://t.co/WJs5bmRthU,\") == \", ,\"\n",
    "    assert process_tweet(\"hellohttp://t.co/WJs5bmRthUworld\") == \"hello world\"\n",
    "    assert process_tweet(\"http://taco/WJs5bmRthU\") == \"http://taco/WJs5bmRthU\"\n",
    "    assert process_tweet(\n",
    "        'Protect your child from #CoronavirusOutbreak.\\n\\nhttps://t.co/qPREVvM2C5\\n\\n#CoronaVirusUpdate #COVID2019 #COVID #Coronavid19 #outbreak #Italy #COVIDãƒ¼19 #BeSafe #Containment #Homeschooling #DigitalTransformation #InternationalSchooling #virtualschool #OnlineNOW #edtech #technology'\n",
    "    ) == \"Protect your child from #CoronavirusOutbreak.\\n\\n \\n\\n#CoronaVirusUpdate #COVID2019 #COVID #Coronavid19 #outbreak #Italy #COVIDãƒ¼19 #BeSafe #Containment #Homeschooling #DigitalTransformation #InternationalSchooling #virtualschool #OnlineNOW #edtech #technology\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_process_tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm1MU9dI0wrL"
   },
   "source": [
    "## Part B: Process Web Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2w-gTJp0wrM"
   },
   "source": [
    "Let's now move to extracting text from web articles, using Beautifulsoup to parse HTML data. More specifically, we have collected news articles related to the same topic of Coronavirus from [Nature](https://www.nature.com/), and want to also wrangle this data accordingly. Through this exercise, you will learn how to navigate HTML structures from different webpages in order to get the desired information.\n",
    "\n",
    "To begin, we have provided you a helper function `retrieve_url` takes as input a webpage string URL and creates a BeautifulSoup object from the corresponding page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qe4mtqgh5CxC"
   },
   "outputs": [],
   "source": [
    "def retrieve_url(local_file_location):\n",
    "  with open(local_file_location, 'r', encoding='utf-8') as mf:\n",
    "    html_content = mf.read()\n",
    "  soup = BeautifulSoup(html_content, 'html.parser')\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R_I88IH0wrM"
   },
   "source": [
    "### Question 2: Parsing a single article from Nature\n",
    "Implement the function `parse_page_nature` that takes as input a path pointing to a text dump of a Nature news article, and returns a JSON dictionary with the following format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Title': 'When will the coronavirus outbreak peak?' #str\n",
    "    'Author': ['David Cyranoski'] # list, a list of author names in the same order as they appear on the page\n",
    "    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n",
    "    'Summary': '.....' #str, the summary div between the title and author fields, or empty string if no summary is available\n",
    "    'Content': ['.....'] #Any content that follows the author fields, or empty string if no content is available.\n",
    "}\n",
    "```\n",
    "\n",
    "The values of `Summary` and `Content` should be raw texts that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b><a href=\"https://google.com\">World</a><p>\"` then the output `Content` should be `\"Hello World\"`.\n",
    "\n",
    "In the local test we have provided the full reference JSON files for some article pages. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n",
    "\n",
    "**Notes**:\n",
    "* Occasionally there are some \"Related\" blocks embedded in the article text (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nature_related.png)). These are characterized by the attribute `data-label=\"Related\"` and should **not** be included in the parsing result.\n",
    "* The `Published Date` field should be the original article date, not the updated date. For example, the `Published Date` for [this article](https://www.nature.com/articles/d41586-020-00166-6) is 2020-01-22.\n",
    "* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A.\n",
    "* Do not parse information form the `meta` tags as they are not robust. Every required information can be found within `body`.\n",
    "* If an article has no authors (e.g., https://www.nature.com/articles/d41586-020-00589-1), the Author field should be an empty list.\n",
    "* For the Content list, only text contents that come from the `p` tags in the article body should be included. You can start by identifying a `div` that corresponds to the entire article body (looking at the CSS class names may be helpful). Note that if an image caption is the child of a `p` tag, its content should be included as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "oFa1fdUx0wrM"
   },
   "outputs": [],
   "source": [
    "def remove_unused_content(soup):\n",
    "    for aside in soup.find_all(True, {\"data-label\" : \"Related\"}):\n",
    "        aside.extract()\n",
    "\n",
    "\n",
    "def parse_page_nature(url):\n",
    "    \"\"\"\n",
    "    Parse a single New York Times article at the given URL\n",
    "\n",
    "    args:\n",
    "        url (str) : the article URL\n",
    "\n",
    "    return:\n",
    "        Dict[str, str] : the parsed information stored in JSON format, which includes:\n",
    "            Title, Author, Published Date, Summary and Content\n",
    "    \"\"\"\n",
    "    soup = retrieve_url(url)\n",
    "    remove_unused_content(soup)\n",
    "    result = {}\n",
    "\n",
    "    result[\"Title\"] = soup.find(\"h1\", class_ = \"c-article-magazine-title\").get_text().strip()\n",
    "    \n",
    "    result[\"Content\"] = []\n",
    "\n",
    "    if soup.find(\"p\", class_ = \"article__teaser\") is not None:\n",
    "      for p in soup.find(\"p\", class_ = \"article__teaser\").find_all(\"p\"):\n",
    "          for child in p.find_all():\n",
    "              child.unwrap()\n",
    "          content = p.get_text().strip()\n",
    "          if len(content) > 0:\n",
    "              result[\"Content\"].append(content)\n",
    "\n",
    "    body = soup.find(\"div\", class_ = \"c-article-body\")\n",
    "    if body is not None:\n",
    "      for p in body.find_all(\"p\"):\n",
    "          for child in p.find_all():\n",
    "              child.unwrap()\n",
    "          content = p.get_text().strip()\n",
    "          if len(content) > 0:\n",
    "              result[\"Content\"].append(content)\n",
    "    \n",
    "    # TODO: Add the Author field to the result dict.\n",
    "    result[\"Author\"] = [\" \".join(author.get(\"content\").split(\", \")[::-1]) for author in soup.find_all(\"meta\", attrs={\"name\": \"dc.creator\"})]\n",
    "\n",
    "    # TODO: Add the Summary field to the result dict.\n",
    "    result[\"Summary\"] = soup.find(\"meta\", attrs={\"name\": \"description\"}).get(\"content\")\n",
    "\n",
    "    # TODO: Add the Published Date field to the result dict.\n",
    "    result[\"Published Date\"] = soup.find(\"meta\", attrs={\"name\": \"dc.date\"}).get(\"content\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Cr7AZx00wrM",
    "outputId": "dc04d9f9-5e56-43a1-b789-7f1661db87e1",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'China coronavirus: Six questions scientists are asking', 'Author': ['Ewen Callaway', 'David Cyranoski'], 'Summary': 'Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.', 'Published Date': '2020-01-22', 'Content': []}\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_parse_page_nature():\n",
    "    nature0 = parse_page_nature(\"html_data/nature_0.html\")\n",
    "    nature0_ref = json.load(open('local_test_refs/0_ref.txt'))\n",
    "    print(nature0_ref)\n",
    "    assert nature0 == nature0_ref\n",
    "\n",
    "    nature1 = parse_page_nature(\"html_data/nature_1.html\")\n",
    "    nature1_ref = json.load(open('local_test_refs/1_ref.txt'))\n",
    "    assert nature1 == nature1_ref\n",
    "\n",
    "    nature2 = parse_page_nature(\"html_data/nature_2.html\")\n",
    "    nature2_ref = json.load(open('local_test_refs/2_ref.txt'))\n",
    "    assert nature2 == nature2_ref\n",
    "\n",
    "    nature3 = parse_page_nature(\"html_data/nature_3.html\")\n",
    "    nature3_ref = json.load(open('local_test_refs/3_ref.txt'))\n",
    "    assert nature3 == nature3_ref\n",
    "\n",
    "\n",
    "    nature4 = parse_page_nature(\"html_data/nature_4.html\")\n",
    "    nature4_ref = json.load(open('local_test_refs/4_ref.txt'))\n",
    "    assert nature4 == nature4_ref\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_parse_page_nature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0l7Cqxt0wrR"
   },
   "source": [
    "### Question 3: Process news articles data\n",
    "While the JSON data format we constructed earlier is useful for checking the correctness of our parsing, eventually we would like each article to be represented by just a string. For our purpose, we will define the string representation of an article as\n",
    "\n",
    "`\"<title> <summary> <content paragraph 1> <content paragraph 2> <content paragraph 3> ...\"`\n",
    "\n",
    "where there is a single space separating each field (note that the content paragraphs come from the `\"Content\"` field of an article json, which is a list of paragraph strings).\n",
    "\n",
    "Implement the function `process_news_article` that takes as input a JSON dictionary resulting from parsing a Nature or NYT article, and converts the JSON to the above string format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "fDCKrvuX0wrS"
   },
   "outputs": [],
   "source": [
    "def process_news_article(article_json):\n",
    "    \"\"\"\n",
    "    Convert article jsons to nested list of tokens of processed article contents\n",
    "\n",
    "    args:\n",
    "        article_json (Dict[str, str]] : JSON content of a news article\n",
    "\n",
    "    return:\n",
    "        List[str] : a list of processed tokens from the input article JSON\n",
    "    \"\"\"\n",
    "    return f\"{article_json['Title']} {article_json['Summary']} {' '.join([content for content in article_json['Content']])}\"\n",
    "\n",
    "\n",
    "# do not modify this function\n",
    "def process_news_articles_data(article_jsons):\n",
    "    return [process_news_article(article) for article in article_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjQXmgZg0wrS",
    "outputId": "a3c8dc6a-d436-4555-9811-4db45e156b58",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_process_news_article():\n",
    "    nature_article = json.load(open(\"local_test_refs/4_ref.txt\", 'r', encoding='utf-8'))\n",
    "    nature_article_processed = process_news_article(nature_article)\n",
    "    nature_expected = open(\"local_test_refs/nature4_processed.txt\").read()\n",
    "    assert nature_article_processed == nature_expected\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_process_news_article()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDTiSdam0wrS"
   },
   "source": [
    "## Part C: Mining PDF Data\n",
    "Having extracted data from Twitter and newspapers, we now turn to our third source: research papers. We have provided you with 15 pdf files, collected from the [arxiv API](https://arxiv.org/help/api). These are located in the `pdfs` directory and labeled from `arxiv_01.pdf` to `arxiv_15.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgB7wSyz0wrT"
   },
   "source": [
    "### Question 4: Parse a single Arxiv research paper\n",
    "Implement the function `parse_pdf` that takes as input a PDF file path and outputs the processed tokenization of the text content of that file. In particular, you should remove all URLs, i.e., strings that start with \"http://\" or \"https://\".\n",
    "\n",
    "**Notes**:\n",
    "* For this question, you should use the function [`extract_text`](https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#extract-text) from the `pdfminer` package to convert a pdf file to string.\n",
    "* Unlike in the tweet scenario, there is no limit on the length of an URL in this case. The URL pattern you should use here is: a string that starts with `http://` or `https://`, followed by any number of non-space character. Do not make any other assumption (for example, don't assume an URL always contains a `.`).\n",
    "* We have provided a template helper function `remove_url_regex`, where you can enter the regex for removing URLs. There are some local test cases in `test_remove_url_regex` to help you validate your regex. If your regex passes these tests, you can use it in `parse_and_clean_pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "xN22jY990wrT"
   },
   "outputs": [],
   "source": [
    "def remove_url_regex():\n",
    "    # enter your regex for capturing url strings here\n",
    "    # you can call this function in parse_and_clean_pdf()\n",
    "    regex = r\"(?:https?://\\S+)\"\n",
    "    return regex\n",
    "\n",
    "def parse_and_clean_pdf(file):\n",
    "    \"\"\"\n",
    "    Convert an input pdf file into processed and cleaned raw text.\n",
    "\n",
    "    args:\n",
    "        file (str) : the pdf file path\n",
    "\n",
    "    return:\n",
    "        str: the cleaned version of the input file content\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(remove_url_regex(), \"\", high_level.extract_text(file))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxJ2QWYK0wrT",
    "outputId": "283f1f0d-f779-4b69-91ee-1e9111dc7fc3",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_remove_url_regex():\n",
    "    s = \"http://abc\"\n",
    "    assert re.sub(remove_url_regex(), '', s) == ''\n",
    "\n",
    "    s = 'hellohttps://github.com/lanagarmire/COVID19-Drugs-LungInjury'\n",
    "    assert re.sub(remove_url_regex(), '', s) == 'hello'\n",
    "\n",
    "    s = 'http://example.com https://cmu.edu'\n",
    "    assert re.sub(remove_url_regex(), '', s) == ' '\n",
    "\n",
    "    s = 'https://www.'\n",
    "    assert re.sub(remove_url_regex(), '', s) == ''\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_remove_url_regex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yJwkmLx0wrT",
    "outputId": "2c1dbb72-bded-4fa2-df79-b922c9693636",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_parse_and_clean_pdf():\n",
    "    pdf_text = parse_and_clean_pdf(\"pdfs/arxiv_01.pdf\")\n",
    "    with open(\"local_test_refs/parsed_arxiv_01.txt\") as outfile:\n",
    "        assert pdf_text == outfile.read()\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_parse_and_clean_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRARSOnGGoEv"
   },
   "source": [
    "### Question 5: Parse several Arxiv research papers\n",
    "Implement the function `process_arxiv_data` that takes as input the path to a directory. This function parses and cleans all pdf files in that directory, then returns a list of strings, where each string results from parsing one PDF file.\n",
    "\n",
    "**Hint**: You might find using your previous question useful.\n",
    "\n",
    "**Notes**:\n",
    "* The pdf files should be processed based on the alphabetical order of their name, e.g., `arxiv_01.pdf` before `arxiv_02.pdf`.\n",
    "* Do not assume that `os.listdir` will return the filenames in sorted order; you should perform the sorting yourself.\n",
    "* Do not assume every file in the input directory is a pdf file; only those whose names end in `.pdf` should be parsed.\n",
    "* If you fail the test case here, it is likely that your URL removal regex from Question 10 is incorrect. Try to come up with more test cases to test your URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "lKoyv5xcGmO2"
   },
   "outputs": [],
   "source": [
    "def process_arxiv_data(directory, num_files=15):\n",
    "    \"\"\"\n",
    "    Parse and process the text content of all pdf papers in alphabetical order in a given directory\n",
    "\n",
    "    args:\n",
    "        directory (str) : the relative file path to a directory that contains the pdf papers\n",
    "        num_files (int) : Only return the first num_files files\n",
    "\n",
    "    return:\n",
    "        List[List[str]] : a list of list of word tokens\n",
    "    \"\"\"\n",
    "    return [parse_and_clean_pdf(os.path.join(directory, file_path)) for file_path in list(sorted(os.listdir(directory)))[:num_files]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3DE3lE9GsQU",
    "outputId": "afc708ab-a371-46a9-dc38-e80bae412cec",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_process_arxiv_data():\n",
    "    paper_contents = process_arxiv_data(\"pdfs\")\n",
    "    sample_of_contents = [paper[0:100] for paper in paper_contents]\n",
    "\n",
    "    assert len(paper_contents) == 15\n",
    "    assert sample_of_contents == ['Repurposed drugs for treating lung injury in COVID-19 \\n\\nBing He1, Lana Garmire1* \\n\\n1. Department of ', '0\\n2\\n0\\n2\\n\\nr\\na\\n\\nM\\n1\\n3\\n\\n]\\nE\\nP\\n.\\no\\ni\\nb\\n-\\nq\\n[\\n\\n1\\nv\\n4\\n8\\n2\\n4\\n1\\n.\\n3\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nThe fractal time grow', 'Coronavirus and financial volatility: 40 days of fasting and fear \\n\\nClaudiu Tiberiu ALBULESCU1,2\\uf02a \\n\\n', '0\\n2\\n0\\n2\\n\\nr\\na\\n\\nM\\n8\\n\\n]\\nE\\nP\\n.\\no\\ni\\nb\\n-\\nq\\n[\\n\\n1\\nv\\n5\\n7\\n7\\n3\\n0\\n.\\n3\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nData Analysis for the C', 'How  many  infections  of  COVID-19  there  will  be  in  the  “Diamond  Princess”-\\n\\nPredicted by a ', 'Parametric analysis of early data on COVID-19 expansion in selected\\nEuropean countries\\n\\naInstitute o', '0\\n2\\n0\\n2\\n\\nr\\na\\n\\nM\\n5\\n2\\n\\n]\\n\\nR\\n\\nI\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n7\\n0\\n1\\n0\\n0\\n.\\n3\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nViewing the Progression o', 'Insights from early mathematical models of 2019-nCoV acute respiratory disease (COVID-\\n\\nEarly models', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n4\\n2\\n\\n]\\nh\\np\\n-\\nc\\no\\ns\\n.\\ns\\nc\\ni\\ns\\ny\\nh\\np\\n[\\n\\n1\\nv\\n2\\n0\\n3\\n0\\n1\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nThe Recons', 'COVID-19  Docking  Server:  An  interactive  server  for \\n\\ndocking  small  molecules,  peptides  and', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n2\\n2\\n\\n]\\nE\\nP\\n.\\no\\ni\\nb\\n-\\nq\\n[\\n\\n1\\nv\\n0\\n4\\n6\\n9\\n0\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nThe Outbreak Evaluatio', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n5\\n2\\n\\n]\\nh\\np\\n-\\nc\\no\\ns\\n.\\ns\\nc\\ni\\ns\\ny\\nh\\np\\n[\\n\\n2\\nv\\n9\\n9\\n1\\n9\\n0\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nScaling fe', '0\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n2\\n1\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n4\\n3\\n5\\n5\\n0\\n.\\n2\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nABNORMAL RESPIRATORY PATTER', 'Trend and forecasting of the COVID-19 outbreak in\\nChina\\n\\nQiang Li1 Wei Feng2\\n\\n1School of Physical sc', 'Deep  Learning  System  to  Screen  Coronavirus  Disease  2019 \\n\\nPneumonia \\n\\nXiaowei Xu1, MD; Xianga']\n",
    "    assert sum(len(paper) for paper in paper_contents) == 368635\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_process_arxiv_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp2M2EGl0wrD"
   },
   "source": [
    "## Part D: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72Kpyo_I0wrD"
   },
   "source": [
    "Now that we've converted our initial set of structured data into pure text, let's process all of the text in a similar way. Text data on the internet is very messy.  Typically there is a fair amount of processing work to do once you have collected any sizeable chunk of text data, in order to have it ready for subsequent analyses. To get you familiar with this kind of data, this section will walk you through some common processing tasks:\n",
    "\n",
    "The first step is to import the lemmatizer and set of English stopwords from `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "EzYSYJPT0wrE"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet = True)\n",
    "nltk.download(\"wordnet\", quiet = True)\n",
    "nltk.download(\"punkt\", quiet = True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet = True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGdvHTIo0wrF"
   },
   "source": [
    "### Question 6: Text cleaning and tokenization\n",
    "Implement the three functions `clean_string`, `tokenize` and `lemmatize` that perform the following text preprocessing tasks:\n",
    "\n",
    "1. `clean_text` should:\n",
    "    * convert the string to lower case.\n",
    "    * remove any instance of `'s` that is either followed by any whitespace character, or at the end of the string: `teacher's help` becomes `teacher help`, and `children's` becomes `children`.\n",
    "    * remove apostrophe character `'`: `don't` becomes `dont`. For simplicity we will only consider the character `'` as apostrophe (so `’` is not).\n",
    "    * remove leading and trailing space.\n",
    "\n",
    "1. `tokenize` should:\n",
    "    * use `nltk.word_tokenize` to tokenize the input text.\n",
    "    * further break tokens at characters which are not digits 0-9 and not present in `string.ascii_letters`. For example, `a_b_c` becomes `['a', 'b', 'c']`.\n",
    "    * maintain the token order as it appears in the original string.\n",
    "\n",
    "1. `lemmatize` should:\n",
    "    * lemmatize each token individually.\n",
    "    * remove tokens that are stopwords or contain fewer than two characters (these two cases should be checked after the lemmatization step).\n",
    "    \n",
    "**Notes**:\n",
    "* When lemmatizing a word, you should also specify the part-of-speech `pos` parameter. This can be obtained by calling `nltk.pos_tag` and using the first returned tag (in case there are multiple possibilities). You can interpret the returned tag as follows:\n",
    "    * If it starts with \"J\", it is an adjective.\n",
    "    * If it starts with \"V\", it is a verb.\n",
    "    * If it starts with \"R\", it is an adverb.\n",
    "    * Otherwise, it is a noun.\n",
    "* `nltk.pos_tag` should be called on each individual token, instead of on the entire tokenized text. For example, if the input string is `\"learning is fun\"`, you should call `nltk.pos_tag([\"learning\"])` to get the part-of-speech of `'learning'`, and input that to the lemmatizer. You may notice that in this case `\"learning\"` is classified as a verb (while it is a noun in the original sentence). However, this is not a problem, since our end goal is to reduce each token to its base form, not to correctly classify its part-of-speech.\n",
    "* If you use the regex character set `\\w`, note that it matches alphanumeric characters **and** the underscore character `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "tMONw0Qd0wrG"
   },
   "outputs": [],
   "source": [
    "def get_pos(w):\n",
    "    tag = nltk.pos_tag(w)[0][1]\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input string by converting it to lowercase, removing 's and apostrophe.\n",
    "\n",
    "    args:\n",
    "        text (str) : the input text\n",
    "\n",
    "    return:\n",
    "        str : the cleaned text\n",
    "    \"\"\"\n",
    "    cleaned_text = text.lower()\n",
    "    pattern = r\"'s(?=\\s|$)\"\n",
    "    cleaned_text = re.sub(pattern, \"\", cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\"'\", \"\")\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(cleaned_text):\n",
    "    \"\"\"\n",
    "    Tokenize the input string.\n",
    "\n",
    "    args:\n",
    "        cleaned_text (str): the input text, output from clean_text\n",
    "\n",
    "    return:\n",
    "        List[str] : a list of tokens from the input text\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token in nltk.word_tokenize(cleaned_text):\n",
    "        tokens.extend([word for word in re.split(r'[^a-zA-Z0-9]+', token) if word])\n",
    "    return tokens\n",
    "\n",
    "def lemmatize(tokens, stopwords = {}):\n",
    "    \"\"\"\n",
    "    Lemmatize each token in an input list of tokens\n",
    "\n",
    "    args:\n",
    "        tokens (List[str]) : a list of token, output from tokenize\n",
    "\n",
    "    kwargs:\n",
    "        stopwords (Set[str]) : the set of stopwords to exclude\n",
    "\n",
    "    return:\n",
    "        List[str] : a list of lemmatized and filtered tokens\n",
    "    \"\"\"\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, get_pos([token]))\n",
    "        if (len(lemmatized_token) > 1) and (lemmatized_token not in stopwords):\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def preprocess_text(text, stopwords = {}):\n",
    "    # do not modify this function\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize(cleaned_text)\n",
    "    return lemmatize(tokens, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "394jTGgx0wrG",
    "outputId": "703173a7-6534-4bc6-8fc1-08498a888afb",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_preprocess_text():\n",
    "    # cleaning\n",
    "    assert clean_text(\"I like Data Science\") == \"i like data science\"\n",
    "    assert clean_text(\"She's\") == \"she\"\n",
    "    assert clean_text(\"you've\")== \"youve\"\n",
    "    assert clean_text(\"car, cars, car's cars'\")== \"car, cars, car cars\"\n",
    "    assert clean_text(\"'shed'\") == \"shed\"\n",
    "    assert clean_text(\"'good news'\") == \"good news\"\n",
    "    assert clean_text(\"CMU's campus\")== \"cmu campus\"\n",
    "    assert preprocess_text(\"abc 'system\") == ['abc', 'system']\n",
    "    assert preprocess_text(\"O'Shea Jackson Jr. is an American actor and musician\") == ['oshea', 'jackson', 'jr', 'be', 'an', 'american', 'actor', 'and', 'musician']\n",
    "\n",
    "    # tokenization\n",
    "    assert tokenize(\"ab..ab. .ab . ab.\") == ['ab', 'ab', 'ab', 'ab'], tokenize(\"ab..ab. .ab . ab.\")\n",
    "    assert tokenize(\"word-of-mouth hello,world\")== ['word', 'of', 'mouth', 'hello', 'world']\n",
    "    assert tokenize(\"gotta\")== ['got', 'ta']\n",
    "    assert tokenize(\"hello_world\") == [\"hello\", \"world\"]\n",
    "    assert preprocess_text(\"hope this👏will work\") == ['hope', 'this', 'will', 'work']\n",
    "\n",
    "    # lemmatization\n",
    "    assert lemmatize([\"cats\"]) == ['cat']\n",
    "    assert lemmatize([\"did\"]) == ['do']\n",
    "    assert lemmatize([\"learning\", \"is\", \"fun\"], english_stopwords) == [\"learn\", \"fun\"]\n",
    "\n",
    "    # miscellaneous\n",
    "    assert preprocess_text(\"the weather is really nice\", english_stopwords) == ['weather', 'really', 'nice']\n",
    "    assert preprocess_text(\n",
    "        \"To apply SVM learning in partial discharge classification, data input is very important!?\",\n",
    "        english_stopwords\n",
    "    ) == 'apply svm learn partial discharge classification data input important'.split()\n",
    "    assert preprocess_text(\"after all he's done\", english_stopwords) == []\n",
    "    assert preprocess_text(\"they didn’t have much chance of guessing what it was without further clues.\", english_stopwords) == ['much', 'chance', 'guess', 'without', 'far', 'clue']\n",
    "    assert preprocess_text(\"DUQUE'S\", english_stopwords) == [\"duque\"]\n",
    "    assert preprocess_text(\"the 'rona\", english_stopwords) == ['rona']\n",
    "    assert preprocess_text('MOTORCYCLES DONT FLY', english_stopwords)==['motorcycle', 'dont', 'fly']\n",
    "    assert preprocess_text('“ Georg e\\”', english_stopwords) == ['georg']\n",
    "    text = \"Harry leapt into the air; he’d trodden on something big and squashy on the doormat — something alive\"\n",
    "    assert preprocess_text(text, english_stopwords) == ['harry', 'leapt', 'air', 'trodden', 'something', 'big', 'squashy', 'doormat', 'something', 'alive']\n",
    "    assert preprocess_text(\"Donâ€™t want to add to TRUMPâ€™s #COVID19 numbers. #CoronaVirus ðŸ¦  donâ€™t care.\", english_stopwords) == ['want', 'add', 'trump', 'covid19', 'number', 'coronavirus', 'care']\n",
    "\n",
    "    # test on long text string\n",
    "    with open(\"local_test_refs/henrys_letter.txt\", encoding = \"utf-8\") as infile, open(\"local_test_refs/processed_henrys_letter.txt\", encoding = \"utf-8\") as outfile:\n",
    "        processed_str = preprocess_text(infile.read())\n",
    "        reference_str = outfile.read().splitlines()\n",
    "        assert processed_str == reference_str\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_preprocess_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bgvLcWF0wrH"
   },
   "source": [
    "You may notice that the lemmatization functionality isn't perfect; for example, it would map `\"as\"` to `\"a\"` because `\"as\"` is being treated as a noun instead of a proposition (with tag `\"IN\"`). In general, identifying the correct part-of-speech tag is very context-dependent (for example, `\"back\"` can be either an adjective, adverb, verb or noun). In the context of this project, we will not dive deep into these linguistic nuances, and settle with the lemmatization rules above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvfjaodx0wrX"
   },
   "source": [
    "## Part E: Data Visualization and Feature Construction\n",
    "Now that we have collected text data from three different sources (Twitter, news articles and research papers), let's put them all together in order to perform some simple exploratory data analyses and feature construction. From now we will define a *document* as a list of tokens coming from a single tweet, news article or arxiv paper, and a *corpus* as a list of documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOcbOIhY0wrX"
   },
   "source": [
    "### Question 7: Word frequency and word cloud\n",
    "With any text corpus, you will first want to check for the word frequency distribution, in particular which words are the most common and which are the least. The former group may consist of terms that are relevant to the topic, or terms that simply appear frequently in general (e.g., stopwords). The latter group may consist of highly specialized terms or typos. Since stopwords and rare words are not useful to our analysis, we will remove both (where we define rare words as words that only appear *once in the corpus*).\n",
    "\n",
    "Implement the function `word_frequency` which takes as input a text corpus and returns a `collections.Counter` object mapping each word to its frequency in the corpus. However, rare words that only appear once in the entire corpus should **not** be included in this mapping.\n",
    "\n",
    "**Notes**:\n",
    "* Recall that `preprocess_text` already handles stopword removal, so you only need to remove rare words in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "JoATsTY90wrX"
   },
   "outputs": [],
   "source": [
    "def word_frequency(corpus):\n",
    "    \"\"\"\n",
    "    Count the word frequency in a given corpus\n",
    "\n",
    "    args:\n",
    "        corpus (List[List[str]]) : a nested list of tokens, where each inner list is a processed document\n",
    "\n",
    "    return:\n",
    "        collections.Counter : a mapping between each word and its frequency in the corpus, excluding words that\n",
    "            only appear once\n",
    "    \"\"\"\n",
    "    counter = collections.Counter()\n",
    "    for processed_document in corpus:\n",
    "        counter.update(processed_document)\n",
    "\n",
    "    for processed_document in counter.copy(): \n",
    "        if counter[processed_document] <= 1:\n",
    "            counter.pop(processed_document)\n",
    "\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xg4a2b-60wrY",
    "outputId": "b9e36ead-823e-4285-b5f2-c5b742548969",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_word_frequency():\n",
    "    tweet_corpus = [preprocess_text(elem, english_stopwords) for elem in process_tweet_data(\n",
    "         tweet_data\n",
    "    )[:100]]\n",
    "    counter = word_frequency(tweet_corpus)\n",
    "    assert len(counter) == 230\n",
    "    assert counter[\"coronavirus\"] == 37\n",
    "    assert counter[\"coronavirusoutbreak\"] == 74\n",
    "    assert counter.get(\"the\",0) == 0\n",
    "    assert counter[\"say\"] == 4\n",
    "    assert min(counter.values()) == 2\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_word_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76yj4IBo0wrY"
   },
   "source": [
    "Now we will gather all three corpora together; we store them in a global cache to avoid having to construct them more than once. If you make any code change above this point, rerun the following cell to reset the cache. Note that this will take around 10 minutes to run, and that we've purposefully excluded the cell that runs this from the autograder.\n",
    "\n",
    "**IMPORTANT NOTE**: For grading the functions after this point, we will cache 'local_corpus_store.pkl', which is just a simple file that contains all of the data processed through the functions you've written already. If you change anything above, you will need to re-run this cell in order to ensure that grading works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51Vd52oq0wrY",
    "outputId": "af8b1be6-d6d4-4aa5-b5df-e76187190f7f"
   },
   "outputs": [],
   "source": [
    "corpuses = None\n",
    "\n",
    "def get_corpuses():\n",
    "    global corpuses\n",
    "    with open(\"twitter.txt\",'r') as mf:\n",
    "        tweet_data = json.load(mf)\n",
    "    if corpuses is None:\n",
    "        twitter_corpus = [preprocess_text(elem, english_stopwords) for elem in process_tweet_data(\n",
    "         tqdm(tweet_data[:250])\n",
    "    )]\n",
    "        news_corpus = [preprocess_text(elem, english_stopwords) for elem in tqdm(process_news_articles_data(\n",
    "            [parse_page_nature(f'html_data/nature_{digit}.html')  for digit in range(200)]))]\n",
    "        arxiv_corpus = [preprocess_text(elem, english_stopwords) for elem in tqdm(process_arxiv_data(\"pdfs\", num_files=50))]\n",
    "        corpuses = (twitter_corpus, news_corpus, arxiv_corpus)\n",
    "    return corpuses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 62788.98it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.42it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"local_corpus_store.pkl\", \"wb\") as myfile:\n",
    "    import pickle\n",
    "    pickle.dump(get_corpuses(), myfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRtZmxAc0wrY"
   },
   "source": [
    "Let's first compare the frequency of a number of keywords across these three corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "id": "s6Xi4zBf0wrY",
    "outputId": "135f254f-aeb3-4ed2-d501-214c93f33ae8",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell has been taggged with excluded_from_script\n",
    "# it will not be run by the autograder\n",
    "def get_word_frequency_across_corpuses(input_words):\n",
    "    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n",
    "    twitter_corpus_size = sum(len(d) for d in twitter_corpus)\n",
    "    news_corpus_size = sum(len(d) for d in news_corpus)\n",
    "    arxiv_corpus_size = sum(len(d) for d in arxiv_corpus)\n",
    "    twitter_f, news_f, arxiv_f = word_frequency(twitter_corpus), word_frequency(news_corpus), word_frequency(arxiv_corpus)\n",
    "    return pd.DataFrame({\n",
    "        \"Proportion in twitter corpus\" : [twitter_f.get(word, 0) / twitter_corpus_size for word in input_words],\n",
    "        \"Proportion in news corpus\" : [news_f.get(word, 0) / news_corpus_size for word in input_words],\n",
    "        \"Proportion in arxiv corpus\" : [arxiv_f.get(word, 0) / arxiv_corpus_size for word in input_words]\n",
    "    }, index = input_words)\n",
    "\n",
    "df_frequency = get_word_frequency_across_corpuses([\n",
    "    \"coronavirus\", \"covid\", \"case\", \"health\", \"model\", \"say\", \"test\",\n",
    "    \"2020\", \"19\", \"people\", \"vaccine\"\n",
    "])\n",
    "\n",
    "display(df_frequency)\n",
    "\n",
    "df_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNj82vLk0wrY"
   },
   "source": [
    "We see that there are differences across datasets in the relative frequency of each term. \"Coronavirus\" is used most frequently in tweets, \"say\" most frequently in news corpus, and perhaps unsurprisingly, \"model\" most frequently in arxiv papers. The scientific notation of coronavirus, \"covid,\" isn't used in news articles as much, but is equally popular in both tweets and arxiv papers. On the other hand, \"health\" sees most frequent usage in news articles, likely due to health advice-related articles. Feel free to edit the word list above and see what other insights you can derive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJlxkJq30wrZ"
   },
   "source": [
    "We now move to the last step of data collection and preparation: constructing input features to be used for more formal analyses and language modeling. As language modeling will be covered later in the course, here we will only cover two simple feature construction methods: term frequency (TF) and term frequency - inverse document frequency (TF-IDF), and then use them for our initial task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD4rdRII0wrZ"
   },
   "source": [
    "### Feature construction: term frequency (TF)\n",
    "Implement the function `construct_tf_matrix` that takes as input a corpus and outputs a matrix $TF$ where each row corresponds to one document, and each column corresponds to one of the unique words in the entire corpus. $TF_{ij}$ is the number of times word $j$ appears in document $i$. Similar to the previous question, rare words that only appear once in the entire corpus should be removed, i.e., there should be no columns for those words.\n",
    "\n",
    "**Notes**:\n",
    "* The rows should be ordered based on the document ordering in the corpus. Row 0 corresponds to `corpus[0]`, row 1 to `corpus[1]`, and so on.\n",
    "* The columns should be ordered based on the alphabetical order of their corresponding words. Column 0 corresponds to the alphabetically first word in the corpus, column 1 to the alphabetically second word, and so on.\n",
    "* To ensure code efficiency, avoid using too many loops. Take advantage of Pandas and Numpy functionalities.\n",
    "* We expect you to return an int64 as a datatype. Using `.astype(np.int64)` will help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUJsq5390wrZ"
   },
   "outputs": [],
   "source": [
    "def construct_tf_matrix(corpus):\n",
    "    \"\"\"\n",
    "    Construct a term frequency matrix from an input corpus\n",
    "\n",
    "    args:\n",
    "        corpus (List[List[str]]) : a nested list of word tokens, where each inner list is a document\n",
    "\n",
    "    return:\n",
    "        np.array[n_documents, n_words] : the term frequency matrix\n",
    "    \"\"\"\n",
    "    counters = [collections.Counter(doc) for doc in corpus]\n",
    "    df = pd.DataFrame(counters).fillna(0)\n",
    "    return df.loc[:, df.sum(axis = 0) > 1].sort_index(axis = 1).to_numpy(dtype = np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7y6ISpr0wrZ"
   },
   "source": [
    "### Feature construction: term frequency - inverse document frequency (TF-IDF)\n",
    "We can now compute the TF-IDF matrix, which scales the columns of the term frequency matrix by their inverse document frequency. Recall that the inverse document frequency of a word $j$ is computed as\n",
    "$$\\text{IDF}_j = \\log \\left( \\frac{\\# \\text{ of documents}}{\\# \\text{ of documents with word } j} \\right),$$\n",
    "and so the $\\text{TF-IDF}_{ij}$ entry in the tf-idf matrix is computed as\n",
    "$$\\text{TF-IDF}_{ij} = \\text{TF}_{ij} \\times \\text{IDF}_j.$$\n",
    "\n",
    "Implement the function `tf_idf_matrix` which takes as input a TF matrix and outputs the corresponding TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc2KXaiM0wra"
   },
   "outputs": [],
   "source": [
    "def construct_tf_idf_matrix(tf_matrix):\n",
    "    \"\"\"\n",
    "    Compute the term frequency - inverse document frequency in a corpus\n",
    "\n",
    "    args:\n",
    "        tf_matrix (np.array[n_documents, n_words]) : the term frequency document of the corpus\n",
    "\n",
    "    return:\n",
    "        np.array[n_documents, n_words] : the tf-idf matrix\n",
    "    \"\"\"\n",
    "    idf_matrix = np.log(tf_matrix.shape[0] / np.count_nonzero(tf_matrix, axis=0))\n",
    "    return tf_matrix.astype(np.float64) * idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moixUkGrjdJ4"
   },
   "source": [
    "## Dataset Similarity Comparison\n",
    "\n",
    "Now that we have two separate feature construction pipelines, let's evaluate how similar all three of our datasets are in the lense of these feature construction pipelines. To do so, we shall implement a **very** simple model-based metric called PAD.\n",
    "\n",
    "The idea behind PAD is very simple:\n",
    "\n",
    "1. Train a classification model to try to predict the dataset given the prediction scheme.\n",
    "2. Report the classification error on some dataset, **e**\n",
    "3. Compute **2*(1-2e)** as the metric itself.\n",
    "\n",
    "For our classification model, we shall have you implement a simple logistic regression based classifier. It is important to note that, traditionally, this approach uses a different model called a \"Support Vector Machine\" instead.\n",
    "\n",
    "### Logistic Regression-Based Classification\n",
    "\n",
    "Recall from the primer that logistic regression assumes the following hypothesis function:\n",
    "$$h_\\theta(x) = \\sigma(b + \\theta^T x)$$\n",
    "where $\\sigma(z) = (1+e^{-z})^{-1}$ is the sigmoid function.\n",
    "\n",
    "With this hypothesis funtion, input data $X \\in \\mathbb{R}^{n \\times d}$ and output labels $Y \\in \\{0,1\\}^{n}$, logistic regression attempts to minimize the loss function\n",
    "$$\\mathcal{L}(\\theta, b) = -\\frac{1}{2n} \\left[{\\sum_{i=1}^{n}} y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2n} \\|\\theta\\|_2^2,$$\n",
    "where $\\theta \\in \\mathbb{R}^{d}$ is the weight, $b$ is the intercept, and $\\lambda \\ge 0$ is the regularization parameter.\n",
    "\n",
    "This optimization can be carried out by gradient descent. Given a learning rate $\\alpha$, batch gradient descent for training logistic regression consists of two steps:\n",
    "\n",
    "1. Initialize $b = 0$ and $\\theta$ as a vector of 0s.\n",
    "1. Repeat `n_iters` times:\n",
    "\n",
    "\\begin{align}\n",
    "    b & := b - \\alpha \\cdot  \\frac{1}{2n} \\sum_{i=1}^n \\left(h_\\theta(x^{(i)}) - y^{(i)} \\right), \\\\\n",
    "    \\theta & := \\theta - \\alpha \\cdot \\frac{1}{2n} \\cdot \\left[\\sum_{i=1}^n (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} + 2\\lambda \\theta \\right]\n",
    "\\end{align}\n",
    "\n",
    "After training, we can predict the label for a new data point $x$ as\n",
    "$$\\hat y = \\mathbb{1}\\left(h_\\theta(x) \\ge \\frac 1 2 \\right) = \\mathbb{1}(b + \\theta^T x \\ge 0).$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Implement the class `LRClassifier` with 6 methods -- `__init__`, `loss`, `fit`, `get_weights`, `decision_function` and `predict` -- to perform the above tasks. You can create instance variables as you see fit.\n",
    "\n",
    "**Notes**:\n",
    "* A `LRClassifier` instance may be created once and then trained on several datasets. Therefore, you should initialize $b$ and $\\theta$ inside `.fit`, not in `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45rNELuzpjCI"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LRClassifier:\n",
    "    def __init__(self, lam):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        args:\n",
    "            lam (float) : the regularizer value\n",
    "        \"\"\"\n",
    "        self.lam = lam\n",
    "\n",
    "    def loss(self, h, y):\n",
    "        \"\"\"\n",
    "        Compute the average loss L(theta, b) based on the provided formula.\n",
    "\n",
    "        args:\n",
    "            h (np.array[n_samples]) : a vector of hypothesis function values on every input data point,\n",
    "                this is the output of self.decision_function(X)\n",
    "            y (np.array[n_samples]) : the output label vector, containing 0 and 1\n",
    "\n",
    "        return:\n",
    "            np.float64 : the average loss value\n",
    "        \"\"\"\n",
    "        log_loss = np.mean(-y * np.log(h) - (1 - y) * np.log(1 - h)) / 2\n",
    "        regularizer = self.lam*(self.theta**2).sum()/(2*len(y))\n",
    "        return log_loss + regularizer\n",
    "\n",
    "    def fit(self, X, y, n_iters = 100, alpha = 1):\n",
    "        \"\"\"\n",
    "        Train the model weights and intercept term using batch gradient descent.\n",
    "\n",
    "        args:\n",
    "            X (np.array[n_samples, n_dimensions]) : the input data matrix\n",
    "            y (np.array[n_samples]) : the output label vector, containing 0 and 1\n",
    "\n",
    "        kwargs:\n",
    "            n_iters (int) : the number of iterations to train for\n",
    "            alpha (float) : the learning rate\n",
    "\n",
    "        return:\n",
    "            List[np.float64] : a list of length (n_iters + 1) that contains the loss value\n",
    "                before training and after each training iteration\n",
    "        \"\"\"\n",
    "        self.theta, self.b = np.zeros(X.shape[1]), 0\n",
    "        n = len(y)\n",
    "        losses = []\n",
    "        for _ in range(n_iters):\n",
    "            h = self.decision_function(X)\n",
    "            losses.append(self.loss(h, y))\n",
    "            theta_gradient = 1/(2*n) * ((h - y) @ X) + self.lam*self.theta/n\n",
    "            b_gradient = 1/2 * (h - y).mean()\n",
    "            self.theta -= alpha * theta_gradient\n",
    "            self.b -= alpha * b_gradient\n",
    "        h = self.decision_function(X)\n",
    "        losses.append(self.loss(h, y))\n",
    "        return losses\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Get the model weights and intercept term.\n",
    "\n",
    "        return:\n",
    "            Tuple(theta, b):\n",
    "                theta (np.array[n_dimensions]) : the weight vector\n",
    "                b (np.float64) : the intercept term\n",
    "        \"\"\"\n",
    "        return self.theta, self.b\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the hypothesis function values on every input data point.\n",
    "\n",
    "        args:\n",
    "            X (np.array[n_samples, n_dimensions]) : the input data matrix\n",
    "\n",
    "        return:\n",
    "            np.array[n_samples] : a vector of hypothesis function values on every input data point\n",
    "        \"\"\"\n",
    "        return sigmoid(self.b + X @ self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the label of every input data point.\n",
    "\n",
    "        args:\n",
    "            X (np.array[n_samples, n_dimensions]) : the input data matrix\n",
    "\n",
    "        return:\n",
    "            np.array[n_samples] : a vector of predicted output labels for every input data point\n",
    "        \"\"\"\n",
    "        return np.where(self.b + X @ self.theta >= 0, 1, 0)\n",
    "\n",
    "def binary_lr_classifier(lam = 1e-4):\n",
    "    return LRClassifier(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eYmGoMvpjw9",
    "outputId": "b0643a7c-f42f-48dc-bd71-126fcd501c7c",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_binary_lr_classifier():\n",
    "    X = np.array([[-2, 4], [4, 1], [1, 6], [2, 4], [6, 2]])\n",
    "    y = np.array([0, 0, 1, 1, 1])\n",
    "    lr = binary_lr_classifier(lam = 1e-4)\n",
    "\n",
    "    # before gradient descent\n",
    "    losses = lr.fit(X, y, n_iters = 0)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [0, 0])\n",
    "    assert b == 0\n",
    "    assert np.allclose(losses[-1], 0.34657359027997264)\n",
    "    assert np.allclose(lr.decision_function(X), [0.5] * 5)\n",
    "    assert list(lr.predict(X)) == [1] * len(y)\n",
    "\n",
    "    # 1st iteration\n",
    "    losses = lr.fit(X, y, n_iters = 1)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [0.35, 0.35])\n",
    "    assert b == 0.05\n",
    "    assert np.allclose(losses[-1], 0.33351806318231178)\n",
    "    assert np.allclose(lr.decision_function(X), [0.6791786991753931, 0.8581489350995123, 0.9241418199787566, 0.8956687768809987, 0.9453186827840592])\n",
    "    assert list(lr.predict(X)) == [1] * len(y)\n",
    "\n",
    "    # 2 iterations\n",
    "    losses = lr.fit(X, y, n_iters = 2)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [0.20383002, 0.09069029])\n",
    "    assert np.allclose(b, -0.080246)\n",
    "    assert np.allclose(losses[-1], 0.28778446849618766)\n",
    "    assert np.allclose(lr.decision_function(X), [0.4687546229122032, 0.6954586477733905, 0.6609937974719609, 0.6660059656189242, 0.7898655199585818])\n",
    "    assert list(lr.predict(X)) == [0, 1, 1, 1, 1]\n",
    "\n",
    "    # 1000 iterations\n",
    "    losses = lr.fit(X, y, n_iters = 1000)\n",
    "    theta, b = lr.get_params()\n",
    "    assert np.allclose(theta, [1.62475335, 2.97699553])\n",
    "    assert np.allclose(b, -12.016701793625622)\n",
    "    assert np.allclose(losses[-1], 0.0178892651602277)\n",
    "    assert np.allclose(lr.decision_function(X), [0.0336268115487116, 0.07305423924580728, 0.9994304104089492, 0.9585441655688948, 0.9755365947084815])\n",
    "    assert list(lr.predict(X)) == [0, 0, 1, 1, 1]\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_binary_lr_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcRPn3gupr5z"
   },
   "source": [
    "## Proxy A-Distance Calculation\n",
    "\n",
    "Now to pull it all together. Let's implement a function ``calculate_pad_distance`` that does the following:\n",
    "\n",
    "1. Given a matrix of features ``train_X``, and a list of one-hot encoded binary data ``train_y``, train a logistic regression classifier on that data, setting lambda to be `1e-4`.\n",
    "2. Using a test matrix of features ``test_X`` and a list of test labels ``test_y``, calculate the classification error **e**. If the classification error is greater than 0.5, let the classification error be **1 - e** instead, as we can always flip our classifier's ratings.\n",
    "3. Return **2\\*(1-2e)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6wI2GvHprHO"
   },
   "outputs": [],
   "source": [
    "def calculate_pad_distance(train_X, train_y, test_X, test_y):\n",
    "  \"\"\"\n",
    "  Compute the Proxy A-Distance using the LRClassifier\n",
    "\n",
    "  return:\n",
    "      float:\n",
    "        The Proxy A-Distance, training on train_X and train_y and testing on test_X and test_y\n",
    "  \"\"\"\n",
    "  model = LRClassifier(1e-4)\n",
    "  model.fit(train_X, train_y)\n",
    "  error = min(np.mean(model.predict(test_X) == test_y), np.mean(model.predict(test_X) != test_y))\n",
    "  return 2 * (1 - 2*error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4rLGmQGC0wg"
   },
   "source": [
    "With that computed, let's now calculate the PAD between all three of our data sources.\n",
    "\n",
    "In this case, we shall evaluate the PAD for each pair of datasets using just term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqNSZ_aasl3R",
    "outputId": "bbb3af19-104e-45eb-8096-05962c7e2a4c",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "tf = construct_tf_matrix(corpuses[0] + corpuses[1] + corpuses[2])\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[200:350,:]])\n",
    "train_y = np.array([0]*200 + [1]*150)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[350:400,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF between Twitter and News:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[400:450,:]])\n",
    "train_y = np.array([0]*200 + [1]*50)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF between Twitter and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[200:350,:],tf[400:450,:]])\n",
    "train_y = np.array([0]*150 + [1]*50)\n",
    "test_X = np.concatenate([tf[350:400,:],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF between News and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-9MPTzhEbYY"
   },
   "source": [
    "The PAD between Twitter and News is ``1.44``, the PAD between Twitter and Arxiv is ``1.64``, and the PAD between News and Arxiv is ``0.16``.\n",
    "\n",
    "Implicitly, this makes a lot of sense, as News and Arxiv text are largely more similar, as long-form text, compared with Twitter/X posts, which are limited in length.\n",
    "\n",
    "If we use tf-idf, however, we get a different picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJbAW65jE3_0",
    "outputId": "f4c6ca32-b885-4be3-d2ea-c3a4e40f3387",
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "tf = construct_tf_idf_matrix(construct_tf_matrix(corpuses[0] + corpuses[1] + corpuses[2]))\n",
    "\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[200:350,:]])\n",
    "train_y = np.array([0]*200 + [1]*150)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[350:400,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF-IDF between Twitter and News:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[:200, :],tf[400:450,:]])\n",
    "train_y = np.array([0]*200 + [1]*50)\n",
    "test_X = np.concatenate([tf[200:250, :],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF-IDF between Twitter and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n",
    "\n",
    "train_X = np.concatenate([tf[200:350,:],tf[400:450,:]])\n",
    "train_y = np.array([0]*150 + [1]*50)\n",
    "test_X = np.concatenate([tf[350:400,:],tf[450:500,:]])\n",
    "test_y = np.array([0]*50 + [1]*50)\n",
    "print(\"PAD using TF-IDF between News and Arxiv:\", calculate_pad_distance(train_X, train_y, test_X, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTYWRFGkFDaE"
   },
   "source": [
    "Again, the PAD between Twitter and News is ``0.08``, the PAD between Twitter and Arxiv is ``0.76``, and the PAD between News and Arxiv is ``0.28``.\n",
    "\n",
    "As we can see, the PAD is on average smaller when using TF-IDF compared to using TF. This suggests that it is harder to distinguish between the three data sources using TF-IDF, and thus it might be better to use when training a model if we want to ensure that classifier performance is the same between all three data sources."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
